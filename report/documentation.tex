\documentclass{article}

\usepackage{microtype}
\usepackage{hyperref}

\title{COMP1002 Assignment\\Documentation}
\author{Jakob Wyatt\\19477143}

\begin{document}
\maketitle
\pagebreak
\section{Overview}
\label{sec:Overview}
This program is designed to simulate a social network.
The user interface is as specified in the assignment brief,
with additional functionality including graphical representation
of the socical network in interactive mode, and interactive liking/unliking 
of posts.\\
There exists only one post at a time, with a new post being loaded when the current
post has not had any activity in the last timestep. The original poster always
likes their own post. A user can only like a post once.\\
The simulation consists
of timesteps, with a function \texttt{update()} to move between timesteps.\\
The social network consists of a set of users that may follow eachother,
which has been represented in code as a directed graph. Users may not follow themselves,
or follow eachother more than once.\\
The update algorithm works as follows:
\begin{enumerate}
    \item Check that there exists some users that have liked the post in the previous timestep.
            If there are none, the update ends and the next post is loaded.
    \item Iterate through all users who liked the post in the previous timestep.
            Each of their followers is 'exposed' to the post, and have a chance of liking the post.
            This chance is sampled from a Bernoulli distribution with probability
            $\mathit{clamp}(\mathit{prob\_like} \times \mathit{clickbait\_factor}, 0, 1)$.
    \item If a user likes a post in the current timestep, they have a chance of following the
            original poster. This is sampled using the same technique as above, with global probability
            $\mathit{prob\_foll}$.g
\end{enumerate}
Note that in the above algorithm, if a user does not like a post, they may potentially
be exposed to it later via a different friend. This behaviour is intentional, as it incentivises
a highly connected network.

\section{Justification}
In any program, whenever an ADT or algorithm is selected for use, this use must be justified.
Choice of an ADT must help enhance clarity, make implementation of an algorithm easier,
and have good time and space complexity.

The user network uses the \texttt{DSADirectedGraph} class to represent follows between users
as edges. This helps to enhance clarity, as follows are inherently directional in nature.

Operations that must be performed within this directed graph are find/add/remove verticies,
and find/add/remove edges. It should also be noted that within the context of a social network,
there only exists a maximum of one directed edge between any 2 nodes. Duplicate nodes also will not
exist.
To store verticies and edges within this directed graph, the \texttt{DSALinkedList} class was originally
used to store this data. However, this ended up being rather inefficient, with a time complexity of
$O\left(V\right)$ for vertex find/remove and $O\left(E + V\right)$ for edge find/remove.
Although vertex add and edge add are theoretically $O\left(1\right)$, in practice they had a time complexity
equal to vertex find and edge find, as it had to be checked that the vertex/edge did not already exist in the network.

To improve the performance of these operations, a DSAHashTable was used to store the verticies and edges instead.
This gives $O\left(1\right)$
performance for find, insert, and remove operations. Despite the greater overhead, in practice it was found to outperform
the linked list implementation at a graph size of 20, approximately doubling the speed when the number of verticies was 50.

Multiple \texttt{SocialNetworkPost} objects need to be stored within the \texttt{SocialNetwork} class.
The only time all posts need to be accessed
is when the class is queried for popular posts, which requires ordering the posts by likes.
A simple sorting approach will have a time complexity of $O\left(n\log n\right)$. However, this
approach requires resorting all posts every time the function is called.

Storing the posts in a \texttt{DSAHeap} instead maintains
the time complexity of $O\left(n\log n\right)$. However, if \texttt{popularPosts}
is called multiple times, the DSAHeap is already in a sorted state, greatly reducing the runtime.
A class field is used to cache the most recent post.

As well as being stored in the \texttt{DSADirectedGraph}, users are also stored in a \texttt{DSAHeap},
sorted by follower count. This is done to increase the efficiency of the popular users operation,
and gives the same performance gains listed in the previous paragraph.
One disadvantage of this strategy is that it increases the memory footprint. However, it only increases it
by a linear factor, and does not change the space complexity of the program.

Another performance optimization is that user posts are cached in a dedicated linked list per user.
This speeds up the 'list users posts' operation, as it avoids needing to search through every post. On networks with large amounts
of posts, this gives a significant performance increase.

An alternative algorithm to the one described in \autoref{sec:Overview} is to use a breadth first search through the graph instead,
iterating over all the nodes in the network exactly once. The reason why this was eventually decided against was because
the desired behaviour was to expose a user to a post more often if more of their friends interacted with the post.
This emulates behaviour on social media
platforms such as facebook, where a user may see an old post if a friend has recently liked it.

Some other interesting algorithms that were implemented in this program is the calculation of the globally averaged local clustering coefficient, found in the
function \texttt{SocialNetwork.ClusteringCoefficient}. This algorithm determines the clustering of a graph, with a totally random graph
having low clustering coefficient. This is determined by first finding the local clustering coefficient for each node, which is done by counting
the number of connections between a nodes followers, and dividing by the total number of potential connections. This is then averaged over all
verticies and edges in the network. This algorithm has O(n^3) complexity.

To generate test data, a random-walk style algorithm was used. This algorithm was invented by the author, and to their knowledge, does not exist
elsewhere.

To test different social networks with various parameters, a gridsearch algorithm was used. (RN -> RN) This algorithm finds the outcome of all possible
combinations of parameters, and logs them to a file. An advantage to this algorithm is that it is easy to implement.
Disadvantages of this algorithm is that it produces a lot of data, which can be difficult to analyse, and that it takes a long time to run.

\section{Generated Documentation}
\end{document}
